{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb46c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = \"../data/keypoints/\"\n",
    "CONFIG_URL = \"https://raw.githubusercontent.com/AI4Bharat/OpenHands/main/examples/configs/autsl/decoupled_gcn.yaml\"\n",
    "CONFIG_PATH = \"../config/autsl_decoupled_gcn.yaml\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/autsl/sl_gcn/epoch=72-step=64239.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download config if not exists\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    !wget -O {CONFIG_PATH} {CONFIG_URL}\n",
    "    print(f\"Downloaded config to {CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 17:54:53--  https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_vl/autsl_slgcn.zip\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-12-14 17:54:54 ERROR 404: Not Found.\n",
      "\n",
      "unzip:  cannot find or open ../checkpoints/autsl_slgen.zip, ../checkpoints/autsl_slgen.zip.zip or ../checkpoints/autsl_slgen.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../checkpoints\n",
    "\n",
    "!wget -P ../checkpoints https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_vl/autsl_slgcn.zip\n",
    "!unzip ../checkpoints/autsl_slgcn.zip -d ../checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4502af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 18:31:35--  https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_v1/autsl_metadata.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/369090740/252abc9b-bb2d-43f8-8520-27a75c9f7c26?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-14T19%3A09%3A00Z&rscd=attachment%3B+filename%3Dautsl_metadata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-14T18%3A08%3A50Z&ske=2025-12-14T19%3A09%3A00Z&sks=b&skv=2018-11-09&sig=yKcquTuIYQjtR1x37I%2Bem6Giq5FXRF%2FOwrp8TMAoXo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTczNzMzMywibmJmIjoxNzY1NzM3MDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tTGvNtkcGKoMRETSJor3DTDFx4uOP6YQIfCi_jqFC4s&response-content-disposition=attachment%3B%20filename%3Dautsl_metadata.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-14 18:31:35--  https://release-assets.githubusercontent.com/github-production-release-asset/369090740/252abc9b-bb2d-43f8-8520-27a75c9f7c26?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-14T19%3A09%3A00Z&rscd=attachment%3B+filename%3Dautsl_metadata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-14T18%3A08%3A50Z&ske=2025-12-14T19%3A09%3A00Z&sks=b&skv=2018-11-09&sig=yKcquTuIYQjtR1x37I%2Bem6Giq5FXRF%2FOwrp8TMAoXo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTczNzMzMywibmJmIjoxNzY1NzM3MDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tTGvNtkcGKoMRETSJor3DTDFx4uOP6YQIfCi_jqFC4s&response-content-disposition=attachment%3B%20filename%3Dautsl_metadata.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155640 (152K) [application/octet-stream]\n",
      "Saving to: ‘../data/AUTSL/autsl_metadata.zip.1’\n",
      "\n",
      "autsl_metadata.zip. 100%[===================>] 151.99K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-12-14 18:31:35 (4.06 MB/s) - ‘../data/AUTSL/autsl_metadata.zip.1’ saved [155640/155640]\n",
      "\n",
      "Archive:  ../data/AUTSL/autsl_metadata.zip\n",
      "replace ../data/AUTSL/AUTSL/train_labels.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../data/AUTSL\n",
    "!wget -P ../data/AUTSL https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_v1/autsl_metadata.zip\n",
    "!unzip ../data/AUTSL/autsl_metadata.zip -d ../data/AUTSL\n",
    "!mv ../data/AUTSL/AUTSL ../data/AUTSL/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdfd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1411M  100 1411M    0     0  20.1M      0  0:01:10  0:01:10 --:--:-- 20.7M\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ../data/AUTSL/AUTSL.zip \"https://zenodo.org/records/6674324/files/AUTSL.zip?download=1\"\n",
    "!unzip ../data/AUTSL/AUTSL.zip -d ../data/AUTSL -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a1cceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/AUTSL/**.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976cada",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The pretrained SL-GCN model on AUTSL uses a specific set of 27 keypoints (Body + Hands).\n",
    "However, the `mediapipe_extract.py` script typically extracts keypoints in the order: `Body (33) + Face (468) + Left Hand (21) + Right Hand (21)`.\n",
    "\n",
    "We need to:\n",
    "1.  Reorder the data to `Body + Left Hand + Right Hand`.\n",
    "2.  Select the specific 27 keypoints used by the model.\n",
    "3.  Normalize the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b650ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9920/1820038811.py\", line 1, in <module>\n",
      "    from openhands.datasets.pose_transforms import PoseSelect, CenterAndScaleNormalize\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/datasets/__init__.py\", line 1, in <module>\n",
      "    from .isolated import (\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/datasets/isolated/__init__.py\", line 1, in <module>\n",
      "    from .autsl import AUTSLDataset\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/datasets/isolated/autsl.py\", line 3, in <module>\n",
      "    from .base import BaseIsolatedDataset\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/datasets/isolated/base.py\", line 3, in <module>\n",
      "    import torchvision\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openhands.datasets.pose_transforms import PoseSelect, CenterAndScaleNormalize\n",
    "import omegaconf\n",
    "from openhands.apis.inference import InferenceModel\n",
    "\n",
    "def preprocess_keypoints(file_path):\n",
    "    # Load data\n",
    "    data = np.load(file_path) # Shape: (T, 543, 3)\n",
    "\n",
    "    # Fix: Ensure data has 3 channels (X, Y, Z).\n",
    "    # The model expects 3 channels (81 elements / 27 keypoints = 3).\n",
    "    # If the input only has X, Y, we pad with Z=0.\n",
    "    if data.shape[-1] == 2:\n",
    "        print(f\"Input data has 2 channels. Padding with Z=0 to match model expectation (3 channels).\")\n",
    "        zeros = np.zeros((data.shape[0], data.shape[1], 1))\n",
    "        data = np.concatenate([data, zeros], axis=-1)\n",
    "\n",
    "    # 1. Convert 543 (Body+Face+Hands) -> 75 (Body+Hands)\n",
    "    # The preset 'mediapipe_holistic_minimal_27' assumes indices based on this 75-point layout\n",
    "    # 0-32: Body, 33-53: Left Hand, 54-74: Right Hand\n",
    "    body = data[:, :33, :]\n",
    "    lh = data[:, 501:522, :]\n",
    "    rh = data[:, 522:543, :]\n",
    "    data_75 = np.concatenate([body, lh, rh], axis=1) # (T, 75, 3)\n",
    "\n",
    "    # 2. Convert to Tensor (C, T, V)\n",
    "    # OpenHands transforms expect (C, T, V)\n",
    "    tensor_data = torch.tensor(data_75, dtype=torch.float32).permute(2, 0, 1)\n",
    "    tensor_data = tensor_data[:2, :, :]\n",
    "    sample = {\"frames\": tensor_data}\n",
    "\n",
    "    # 3. Apply OpenHands Transforms using Presets\n",
    "\n",
    "    # Select 27 keypoints using the preset\n",
    "    pose_select = PoseSelect(preset=\"mediapipe_holistic_minimal_27\")\n",
    "    sample = pose_select(sample)\n",
    "\n",
    "    # Normalize using the shoulder preset\n",
    "    normalizer = CenterAndScaleNormalize(reference_points_preset=\"shoulder_mediapipe_holistic_minimal_27\")\n",
    "\n",
    "\n",
    "\n",
    "    # Apply normalization\n",
    "    sample = normalizer(sample)\n",
    "\n",
    "    return sample[\"frames\"] # Returns (C, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263964a6",
   "metadata": {},
   "source": [
    "## Load Model and Extract Features\n",
    "\n",
    "We will load the model using the config and checkpoint, then run the encoder part to get the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5aefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path, checkpoint_path):\n",
    "    cfg = omegaconf.OmegaConf.load(config_path)\n",
    "\n",
    "    # Ensure the config points to the checkpoint\n",
    "    cfg.pretrained = checkpoint_path\n",
    "\n",
    "    # Initialize model\n",
    "    # We use InferenceModel wrapper to handle loading easily\n",
    "    inference_model = InferenceModel(cfg=cfg)\n",
    "    inference_model.init_from_checkpoint_if_available()\n",
    "\n",
    "    # Return the underlying encoder\n",
    "    return inference_model.model.encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8750ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 226 classes in ['train'] splits\n",
      "Loading checkpoint from: ../checkpoints/autsl/sl_gcn/epoch=72-step=64239.ckpt\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/keypoints/signer1_label28_sample15.npy...\n",
      "Input tensor shape: torch.Size([1, 2, 82, 27])\n",
      "Features shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Note: Ensure you have the checkpoint file at CHECKPOINT_PATH\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    encoder = load_model(CONFIG_PATH, CHECKPOINT_PATH)\n",
    "    encoder.to(DEVICE)\n",
    "    encoder.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Process a sample file\n",
    "    # Find a sample file\n",
    "    sample_file = None\n",
    "    for f in os.listdir(DATA_PATH):\n",
    "        if f.endswith(\".npy\"):\n",
    "            sample_file = os.path.join(DATA_PATH, f)\n",
    "            break\n",
    "\n",
    "    if sample_file:\n",
    "        print(f\"Processing {sample_file}...\")\n",
    "        processed_data = preprocess_keypoints(sample_file) # (C, T, V)\n",
    "\n",
    "        # Add batch dimension: (1, C, T, V)\n",
    "        input_tensor = processed_data.unsqueeze(0)\n",
    "\n",
    "        # check shape\n",
    "        print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            input_tensor = input_tensor.to(DEVICE)\n",
    "            features = encoder(input_tensor)\n",
    "\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        # Expected shape: (1, 256) or similar depending on n_out_features\n",
    "    else:\n",
    "        print(\"No .npy files found in data directory.\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}. Please download the pretrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c438f",
   "metadata": {},
   "source": [
    "# Batch Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e649e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 .npy files for feature extraction under /teamspace/studios/this_studio/fsl-bisindo/data/keypoints.\n",
      "Metadata loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "\n",
    "METADATA_URL = \"https://raw.githubusercontent.com/AceKinnn/WL-BISINDO/refs/heads/main/data_structuring/SI_split_metadata.json\"\n",
    "data_root = Path(DATA_PATH)\n",
    "files = sorted(data_root.glob(\"*.npy\"))\n",
    "print(f\"Found {len(files)} .npy files for feature extraction under {data_root.resolve()}.\")\n",
    "\n",
    "\n",
    "with urllib.request.urlopen(METADATA_URL) as url:\n",
    "    metadata = json.loads(url.read().decode())\n",
    "print(\"Metadata loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d834db70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built VIDEO_INDEX with 1600 video_id entries.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "def _iter_metadata_entries(metadata_obj: Any) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Supports common JSON shapes:\n",
    "    - list[entry]\n",
    "    - dict[label_or_key] -> entry\n",
    "    \"\"\"\n",
    "    if isinstance(metadata_obj, list):\n",
    "        return metadata_obj\n",
    "    if isinstance(metadata_obj, dict):\n",
    "        # could be {\"0\": {...}, \"1\": {...}} or {\"entries\": [...]}\n",
    "        if \"entries\" in metadata_obj and isinstance(metadata_obj[\"entries\"], list):\n",
    "            return metadata_obj[\"entries\"]\n",
    "        return list(metadata_obj.values())\n",
    "    raise TypeError(f\"Unsupported metadata type: {type(metadata_obj)}\")\n",
    "\n",
    "def build_video_index(metadata_obj: Any) -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      index[video_id] = {\"label\": int, \"split\": str, \"gloss\": str}\n",
    "    \"\"\"\n",
    "    index: dict[str, dict[str, Any]] = {}\n",
    "\n",
    "    for entry in _iter_metadata_entries(metadata_obj):\n",
    "        gloss = entry.get(\"gloss\", None)\n",
    "        label = entry.get(\"label\", None)\n",
    "        instances = entry.get(\"instances\", []) or []\n",
    "\n",
    "        if label is None or gloss is None:\n",
    "            continue\n",
    "\n",
    "        for inst in instances:\n",
    "            vid = inst.get(\"video_id\", None)\n",
    "            split = inst.get(\"split\", None)\n",
    "            if not vid or not split:\n",
    "                continue\n",
    "            index[str(vid)] = {\"label\": int(label), \"split\": str(split), \"gloss\": str(gloss)}\n",
    "\n",
    "    return index\n",
    "\n",
    "VIDEO_INDEX = build_video_index(metadata)\n",
    "print(f\"Built VIDEO_INDEX with {len(VIDEO_INDEX)} video_id entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df0e0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_from_metadata(path: Path) -> dict | None:\n",
    "    vid = path.stem # filename without .npy\n",
    "    return VIDEO_INDEX.get(vid, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96fd8cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1600/1600 [00:56<00:00, 28.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: torch.Size([1600, 256]) | skipped (no metadata match): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = encoder.to(DEVICE)\n",
    "encoder.eval()\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "splits_list = []\n",
    "video_ids = []\n",
    "paths = []\n",
    "skipped = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p in tqdm(files, desc=\"Extracting features\"):\n",
    "        meta = infer_from_metadata(p)\n",
    "        if meta is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        frames = preprocess_keypoints(str(p)) # (C, T, V)\n",
    "        x = frames.unsqueeze(0).to(DEVICE) # (1, C, T, V)\n",
    "\n",
    "        feat = encoder(x)\n",
    "        feat = feat.reshape(feat.size(0), -1)[0] # (feature_dim,) or (D,)\n",
    "\n",
    "        features_list.append(feat.detach().cpu().to(torch.float32))\n",
    "        labels_list.append(int(meta[\"label\"]))\n",
    "        splits_list.append(str(meta[\"split\"]))\n",
    "        video_ids.append(p.stem)\n",
    "        paths.append(str(p))\n",
    "\n",
    "features = torch.stack(features_list, dim=0) if len(features_list) else torch.empty((0, 0))\n",
    "labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "print(f\"Extracted: {features.shape} | skipped (no metadata match): {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "317f1c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to: /teamspace/studios/this_studio/fsl-bisindo/data/features/slgcn_features_autsl_style.pt\n"
     ]
    }
   ],
   "source": [
    "# Save in a PyTorch-friendly single file for ProtoNet training\n",
    "out_dir = Path(\"../data/features\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"slgcn_features_autsl_style.pt\"\n",
    "\n",
    "# Also store label->gloss for convenience\n",
    "label_to_gloss = {}\n",
    "for v in VIDEO_INDEX.values():\n",
    "    label_to_gloss[int(v[\"label\"])] = v[\"gloss\"]\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"features\": features,          # FloatTensor [N, D]\n",
    "        \"labels\": labels,              # LongTensor [N]\n",
    "        \"splits\": splits_list,         # list[str] aligned with N\n",
    "        \"video_ids\": video_ids,        # list[str] aligned with N\n",
    "        \"paths\": paths,                # list[str] aligned with N\n",
    "        \"label_to_gloss\": label_to_gloss,\n",
    "        \"meta\": {\n",
    "            \"data_root\": str(data_root.resolve()),\n",
    "            \"config_path\": CONFIG_PATH,\n",
    "            \"checkpoint_path\": CHECKPOINT_PATH,\n",
    "            \"num_skipped_no_metadata\": skipped,\n",
    "        },\n",
    "    },\n",
    "    out_path,\n",
    ")\n",
    "\n",
    "print(f\"Saved features to: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adc019a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1600, 256]), torch.Size([1600]), 1600)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test reload\n",
    "\n",
    "blob = torch.load(\"../data/features/slgcn_features_autsl_style.pt\", map_location=\"cpu\")\n",
    "X = blob[\"features\"]     # (N, D)\n",
    "y = blob[\"labels\"]       # (N,)\n",
    "splits = blob[\"splits\"]  # list[str]\n",
    "\n",
    "X.shape, y.shape, len(splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

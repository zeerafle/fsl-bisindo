{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb46c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/imports.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_12378/233312966.py\", line 5, in <module>\n",
      "    from openhands.apis.inference import InferenceModel\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/apis/__init__.py\", line 1, in <module>\n",
      "    from .classification_model import ClassificationModel\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/openhands/apis/classification_model.py\", line 3, in <module>\n",
      "    import torchmetrics\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/__init__.py\", line 14, in <module>\n",
      "    from torchmetrics import functional  # noqa: E402\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/functional/__init__.py\", line 14, in <module>\n",
      "    from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/functional/audio/__init__.py\", line 14, in <module>\n",
      "    from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate  # noqa: F401\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/functional/audio/pit.py\", line 22, in <module>\n",
      "    from torchmetrics.utilities.imports import _SCIPY_AVAILABLE\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/__init__.py\", line 1, in <module>\n",
      "    from torchmetrics.utilities.checks import check_forward_full_state_property  # noqa: F401\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/checks.py\", line 25, in <module>\n",
      "    from torchmetrics.utilities.data import select_topk, to_onehot\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/data.py\", line 19, in <module>\n",
      "    from torchmetrics.utilities.imports import _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/imports.py\", line 112, in <module>\n",
      "    _TORCHVISION_GREATER_EQUAL_0_8: Optional[bool] = _compare_version(\"torchvision\", operator.ge, \"0.8.0\")\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/imports.py\", line 78, in _compare_version\n",
      "    if not _module_available(package):\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchmetrics/utilities/imports.py\", line 59, in _module_available\n",
      "    module = import_module(module_names[0])\n",
      "  File \"/system/conda/miniconda3/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/models/detection/anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/teamspace/studios/this_studio/fsl-bisindo/.venv/lib/python3.11/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = \"../data/keypoints/\"\n",
    "CONFIG_URL = \"https://raw.githubusercontent.com/AI4Bharat/OpenHands/main/examples/configs/autsl/decoupled_gcn.yaml\"\n",
    "CONFIG_PATH = \"../config/autsl_decoupled_gcn.yaml\"\n",
    "CHECKPOINT_PATH = \"../checkpoints/autsl/sl_gcn/epoch=72-step=64239.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download config if not exists\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    !wget -O {CONFIG_PATH} {CONFIG_URL}\n",
    "    print(f\"Downloaded config to {CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780c022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 17:54:53--  https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_vl/autsl_slgcn.zip\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2025-12-14 17:54:54 ERROR 404: Not Found.\n",
      "\n",
      "unzip:  cannot find or open ../checkpoints/autsl_slgen.zip, ../checkpoints/autsl_slgen.zip.zip or ../checkpoints/autsl_slgen.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../checkpoints\n",
    "\n",
    "!wget -P ../checkpoints https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_vl/autsl_slgcn.zip\n",
    "!unzip ../checkpoints/autsl_slgcn.zip -d ../checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4502af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-14 18:31:35--  https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_v1/autsl_metadata.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://release-assets.githubusercontent.com/github-production-release-asset/369090740/252abc9b-bb2d-43f8-8520-27a75c9f7c26?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-14T19%3A09%3A00Z&rscd=attachment%3B+filename%3Dautsl_metadata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-14T18%3A08%3A50Z&ske=2025-12-14T19%3A09%3A00Z&sks=b&skv=2018-11-09&sig=yKcquTuIYQjtR1x37I%2Bem6Giq5FXRF%2FOwrp8TMAoXo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTczNzMzMywibmJmIjoxNzY1NzM3MDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tTGvNtkcGKoMRETSJor3DTDFx4uOP6YQIfCi_jqFC4s&response-content-disposition=attachment%3B%20filename%3Dautsl_metadata.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-12-14 18:31:35--  https://release-assets.githubusercontent.com/github-production-release-asset/369090740/252abc9b-bb2d-43f8-8520-27a75c9f7c26?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-14T19%3A09%3A00Z&rscd=attachment%3B+filename%3Dautsl_metadata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-14T18%3A08%3A50Z&ske=2025-12-14T19%3A09%3A00Z&sks=b&skv=2018-11-09&sig=yKcquTuIYQjtR1x37I%2Bem6Giq5FXRF%2FOwrp8TMAoXo4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTczNzMzMywibmJmIjoxNzY1NzM3MDMzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.tTGvNtkcGKoMRETSJor3DTDFx4uOP6YQIfCi_jqFC4s&response-content-disposition=attachment%3B%20filename%3Dautsl_metadata.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 155640 (152K) [application/octet-stream]\n",
      "Saving to: ‘../data/AUTSL/autsl_metadata.zip.1’\n",
      "\n",
      "autsl_metadata.zip. 100%[===================>] 151.99K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-12-14 18:31:35 (4.06 MB/s) - ‘../data/AUTSL/autsl_metadata.zip.1’ saved [155640/155640]\n",
      "\n",
      "Archive:  ../data/AUTSL/autsl_metadata.zip\n",
      "replace ../data/AUTSL/AUTSL/train_labels.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../data/AUTSL\n",
    "!wget -P ../data/AUTSL https://github.com/AI4Bharat/OpenHands/releases/download/checkpoints_v1/autsl_metadata.zip\n",
    "!unzip ../data/AUTSL/autsl_metadata.zip -d ../data/AUTSL\n",
    "!mv ../data/AUTSL/AUTSL ../data/AUTSL/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fdfd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1411M  100 1411M    0     0  20.1M      0  0:01:10  0:01:10 --:--:-- 20.7M\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ../data/AUTSL/AUTSL.zip \"https://zenodo.org/records/6674324/files/AUTSL.zip?download=1\"\n",
    "!unzip ../data/AUTSL/AUTSL.zip -d ../data/AUTSL -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a1cceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../data/AUTSL/**.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976cada",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The pretrained SL-GCN model on AUTSL uses a specific set of 27 keypoints (Body + Hands).\n",
    "However, the `mediapipe_extract.py` script typically extracts keypoints in the order: `Body (33) + Face (468) + Left Hand (21) + Right Hand (21)`.\n",
    "\n",
    "We need to:\n",
    "1.  Reorder the data to `Body + Left Hand + Right Hand`.\n",
    "2.  Select the specific 27 keypoints used by the model.\n",
    "3.  Normalize the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b650ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_and_select_keypoints(data):\n",
    "    \"\"\"\n",
    "    Reorders data from [Body, Face, LH, RH] to [Body, LH, RH] and selects 27 keypoints.\n",
    "    \"\"\"\n",
    "    # Indices for the 27 keypoints (from OpenHands 'mediapipe_holistic_minimal_27' preset)\n",
    "    # These indices assume the data is ordered as [Body(33), LH(21), RH(21)]\n",
    "    MINIMAL_27_INDICES = [\n",
    "        0, 2, 5, 11, 12, 13, 14, 33, 37, 38, 41, 42, 45, 46, 49, 50, 53, 54,\n",
    "        58, 59, 62, 63, 66, 67, 70, 71, 74\n",
    "    ]\n",
    "\n",
    "    # Data shape: (T, 543, 3)\n",
    "    # 0-32: Body\n",
    "    # 33-500: Face\n",
    "    # 501-521: Left Hand\n",
    "    # 522-542: Right Hand\n",
    "\n",
    "    body = data[:, :33, :]\n",
    "    lh = data[:, 501:522, :]\n",
    "    rh = data[:, 522:543, :]\n",
    "\n",
    "    # Concatenate to form [Body, LH, RH] (T, 75, 3)\n",
    "    combined = np.concatenate([body, lh, rh], axis=1)\n",
    "\n",
    "    # Debug: Print shapes\n",
    "    print(f\"Body shape: {body.shape}\")\n",
    "    print(f\"LH shape: {lh.shape}\")\n",
    "    print(f\"RH shape: {rh.shape}\")\n",
    "    print(f\"Combined shape: {combined.shape}\")\n",
    "    print(f\"Number of indices to select: {len(MINIMAL_27_INDICES)}\")\n",
    "    print(f\"Max index: {max(MINIMAL_27_INDICES)}, Combined V dim: {combined.shape[1]}\")\n",
    "\n",
    "    # Select the 27 keypoints\n",
    "    selected = combined[:, MINIMAL_27_INDICES, :]\n",
    "    print(f\"Selected shape: {selected.shape}\")\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "from openhands.datasets.pose_transforms import PoseSelect, CenterAndScaleNormalize\n",
    "import omegaconf\n",
    "from openhands.apis.inference import InferenceModel\n",
    "\n",
    "def preprocess_keypoints(file_path):\n",
    "    # Load data\n",
    "    data = np.load(file_path) # Shape: (T, 543, 3)\n",
    "\n",
    "    # Fix: Ensure data has 3 channels (X, Y, Z).\n",
    "    # The model expects 3 channels (81 elements / 27 keypoints = 3).\n",
    "    # If the input only has X, Y, we pad with Z=0.\n",
    "    if data.shape[-1] == 2:\n",
    "        print(f\"Input data has 2 channels. Padding with Z=0 to match model expectation (3 channels).\")\n",
    "        zeros = np.zeros((data.shape[0], data.shape[1], 1))\n",
    "        data = np.concatenate([data, zeros], axis=-1)\n",
    "\n",
    "    # 1. Convert 543 (Body+Face+Hands) -> 75 (Body+Hands)\n",
    "    # The preset 'mediapipe_holistic_minimal_27' assumes indices based on this 75-point layout\n",
    "    # 0-32: Body, 33-53: Left Hand, 54-74: Right Hand\n",
    "    body = data[:, :33, :]\n",
    "    lh = data[:, 501:522, :]\n",
    "    rh = data[:, 522:543, :]\n",
    "    data_75 = np.concatenate([body, lh, rh], axis=1) # (T, 75, 3)\n",
    "\n",
    "    # 2. Convert to Tensor (C, T, V)\n",
    "    # OpenHands transforms expect (C, T, V)\n",
    "    tensor_data = torch.tensor(data_75, dtype=torch.float32).permute(2, 0, 1)\n",
    "    tensor_data = tensor_data[:2, :, :]\n",
    "    sample = {\"frames\": tensor_data}\n",
    "\n",
    "    # 3. Apply OpenHands Transforms using Presets\n",
    "\n",
    "    # Select 27 keypoints using the preset\n",
    "    pose_select = PoseSelect(preset=\"mediapipe_holistic_minimal_27\")\n",
    "    sample = pose_select(sample)\n",
    "\n",
    "    # Normalize using the shoulder preset\n",
    "    normalizer = CenterAndScaleNormalize(reference_points_preset=\"shoulder_mediapipe_holistic_minimal_27\")\n",
    "\n",
    "\n",
    "\n",
    "    # Apply normalization\n",
    "    sample = normalizer(sample)\n",
    "\n",
    "    return sample[\"frames\"] # Returns (C, T, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263964a6",
   "metadata": {},
   "source": [
    "## Load Model and Extract Features\n",
    "\n",
    "We will load the model using the config and checkpoint, then run the encoder part to get the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5aefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path, checkpoint_path):\n",
    "    cfg = omegaconf.OmegaConf.load(config_path)\n",
    "\n",
    "    # Ensure the config points to the checkpoint\n",
    "    cfg.pretrained = checkpoint_path\n",
    "\n",
    "    # Initialize model\n",
    "    # We use InferenceModel wrapper to handle loading easily\n",
    "    inference_model = InferenceModel(cfg=cfg)\n",
    "    inference_model.init_from_checkpoint_if_available()\n",
    "\n",
    "    # Return the underlying encoder\n",
    "    return inference_model.model.encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8750ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 226 classes in ['train'] splits\n",
      "Loading checkpoint from: ../checkpoints/autsl/sl_gcn/epoch=72-step=64239.ckpt\n",
      "Model loaded successfully.\n",
      "Processing ../data/keypoints/signer4_label23_sample9.npy...\n",
      "Input tensor shape: torch.Size([1, 2, 50, 27])\n",
      "Features shape: torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Note: Ensure you have the checkpoint file at CHECKPOINT_PATH\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    encoder = load_model(CONFIG_PATH, CHECKPOINT_PATH)\n",
    "    encoder.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Process a sample file\n",
    "    # Find a sample file\n",
    "    sample_file = None\n",
    "    for f in os.listdir(DATA_PATH):\n",
    "        if f.endswith(\".npy\"):\n",
    "            sample_file = os.path.join(DATA_PATH, f)\n",
    "            break\n",
    "\n",
    "    if sample_file:\n",
    "        print(f\"Processing {sample_file}...\")\n",
    "        processed_data = preprocess_keypoints(sample_file) # (C, T, V)\n",
    "\n",
    "        # Add batch dimension: (1, C, T, V)\n",
    "        input_tensor = processed_data.unsqueeze(0)\n",
    "\n",
    "        # check shape\n",
    "        print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            features = encoder(input_tensor)\n",
    "\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        # Expected shape: (1, 256) or similar depending on n_out_features\n",
    "    else:\n",
    "        print(\"No .npy files found in data directory.\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {CHECKPOINT_PATH}. Please download the pretrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c438f",
   "metadata": {},
   "source": [
    "# Batch Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a01f0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
